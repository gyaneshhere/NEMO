{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "835b1f20",
   "metadata": {},
   "source": [
    "![DLI Header](images/DLI_Header.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb717da2",
   "metadata": {},
   "source": [
    "# Token Classification with Large Language Models #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d7f4c5",
   "metadata": {},
   "source": [
    "## Domain-Specific Token Classification Model ##\n",
    "\n",
    "In this notebook, I will fine-tune a pre-trained language model to perform token classification for specific domains. I will develop an NER model that finds disease names in medical disease abstracts. \n",
    "\n",
    "**Table of Contents**<br>\n",
    "This notebook covers the below sections: \n",
    "* Project Overview\n",
    "* Dataset\n",
    "    * Download Data\n",
    "    * Preprocess Data\n",
    "* Fine-Tune a Pre-Trained Model for Custom Domain\n",
    "    * Configuration File\n",
    "    * Download Domain-Specific Pre-Trained Model\n",
    "    * Exercise # 1 - Instantiate Model and Trainer\n",
    "    * Exercise # 2 - Model Training\n",
    "    * Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c2bb65",
   "metadata": {},
   "source": [
    "## Project Overview ##\n",
    "\n",
    "<img src='images/workflow.png' width=1080>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5423f05",
   "metadata": {},
   "source": [
    "## Dataset ##\n",
    "\n",
    "For this notebook, we're going to use the [NCBI-disease](https://www.ncbi.nlm.nih.gov/CBBresearch/Dogan/DISEASE/) corpus, which is a set of 793 PubMed abstracts, annotated by 14 annotators. The annotations take the form of HTML-style tags inserted into the abstract text using the clearly defined rules. The annotations identify named diseases and can be used to fine-tune a language model to identify disease mentions in future abstracts, *whether those diseases were part of the original training set or not*.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40477c66",
   "metadata": {},
   "source": [
    "### Download Data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "85934c3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import wget\n",
    "\n",
    "# set data path\n",
    "DATA_DIR = \"data/NCBI\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2450aade",
   "metadata": {},
   "source": [
    "Here's an example of what an annotated abstract from the corpus looks like: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cf23ea7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288106\tClustering of missense mutations in the <category=\"Modifier\">ataxia-telangiectasia</category> gene in a <category=\"SpecificDisease\">sporadic T-cell leukaemia</category>.\t<category=\"SpecificDisease\">Ataxia-telangiectasia</category> ( <category=\"SpecificDisease\">A-T</category> ) is a <category=\"DiseaseClass\">recessive multi-system disorder</category> caused by mutations in the ATM gene at 11q22-q23 ( ref . 3 ) . The risk of <category=\"DiseaseClass\">cancer</category> , especially <category=\"DiseaseClass\">lymphoid neoplasias</category> , is substantially elevated in <category=\"Modifier\">A-T</category> patients and has long been associated with chromosomal instability . By analysing <category=\"Modifier\">tumour</category> DNA from patients with <category=\"SpecificDisease\">sporadic T-cell prolymphocytic leukaemia</category> ( <category=\"SpecificDisease\">T-PLL</category> ) , a rare <category=\"DiseaseClass\">clonal malignancy</category> with similarities to a <category=\"SpecificDisease\">mature T-cell leukaemia</category> seen in <category=\"SpecificDisease\">A-T</category> , we demonstrate a high frequency of ATM mutations in <category=\"SpecificDisease\">T-PLL</category> . In marked contrast to the ATM mutation pattern in <category=\"SpecificDisease\">A-T</category> , the most frequent nucleotide changes in this <category=\"DiseaseClass\">leukaemia</category> were missense mutations . These clustered in the region corresponding to the kinase domain , which is highly conserved in ATM-related proteins in mouse , yeast and Drosophila . The resulting amino-acid substitutions are predicted to interfere with ATP binding or substrate recognition . Two of seventeen mutated <category=\"SpecificDisease\">T-PLL</category> samples had a previously reported <category=\"Modifier\">A-T</category> allele . In contrast , no mutations were detected in the p53 gene , suggesting that this <category=\"Modifier\">tumour</category> suppressor is not frequently altered in this <category=\"DiseaseClass\">leukaemia</category> . Occasional missense mutations in ATM were also found in <category=\"Modifier\">tumour</category> DNA from patients with <category=\"SpecificDisease\">B-cell non-Hodgkins lymphomas</category> ( <category=\"SpecificDisease\">B-NHL</category> ) and a <category=\"Modifier\">B-NHL</category> cell line . The evidence of a significant proportion of loss-of-function mutations and a complete absence of the normal copy of ATM in the majority of mutated <category=\"DiseaseClass\">tumours</category> establishes somatic inactivation of this gene in the pathogenesis of <category=\"SpecificDisease\">sporadic T-PLL</category> and suggests that ATM acts as a <category=\"Modifier\">tumour</category> suppressor . As constitutional DNA was not available , a putative hereditary predisposition to <category=\"SpecificDisease\">T-PLL</category> will require further investigation . . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(f'{DATA_DIR}/NCBI_corpus_testing.txt') as f: \n",
    "    sample_text=f.readline()\n",
    "    \n",
    "print(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c6c93e",
   "metadata": {},
   "source": [
    "In this example, we see the following tags within the abstract:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1dee35b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<category=\"Modifier\">ataxia-telangiectasia</category>\n",
      "<category=\"SpecificDisease\">sporadic T-cell leukaemia</category>\n",
      "<category=\"SpecificDisease\">Ataxia-telangiectasia</category>\n",
      "<category=\"SpecificDisease\">A-T</category>\n",
      "<category=\"DiseaseClass\">recessive multi-system disorder</category>\n",
      "<category=\"DiseaseClass\">cancer</category>\n",
      "<category=\"DiseaseClass\">lymphoid neoplasias</category>\n",
      "<category=\"Modifier\">A-T</category>\n",
      "<category=\"Modifier\">tumour</category>\n",
      "<category=\"SpecificDisease\">sporadic T-cell prolymphocytic leukaemia</category>\n",
      "<category=\"SpecificDisease\">T-PLL</category>\n",
      "<category=\"DiseaseClass\">clonal malignancy</category>\n",
      "<category=\"SpecificDisease\">mature T-cell leukaemia</category>\n",
      "<category=\"SpecificDisease\">A-T</category>\n",
      "<category=\"SpecificDisease\">T-PLL</category>\n",
      "<category=\"SpecificDisease\">A-T</category>\n",
      "<category=\"DiseaseClass\">leukaemia</category>\n",
      "<category=\"SpecificDisease\">T-PLL</category>\n",
      "<category=\"Modifier\">A-T</category>\n",
      "<category=\"Modifier\">tumour</category>\n",
      "<category=\"DiseaseClass\">leukaemia</category>\n",
      "<category=\"Modifier\">tumour</category>\n",
      "<category=\"SpecificDisease\">B-cell non-Hodgkins lymphomas</category>\n",
      "<category=\"SpecificDisease\">B-NHL</category>\n",
      "<category=\"Modifier\">B-NHL</category>\n",
      "<category=\"DiseaseClass\">tumours</category>\n",
      "<category=\"SpecificDisease\">sporadic T-PLL</category>\n",
      "<category=\"Modifier\">tumour</category>\n",
      "<category=\"SpecificDisease\">T-PLL</category>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# use regular expression to find labels\n",
    "categories=re.findall('<category.*?<\\/category>', sample_text)\n",
    "for sample in categories: \n",
    "    print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6845ec",
   "metadata": {},
   "source": [
    "For our purposes, we will consider any identified category (such as \"Modifier\", \"Specific Disease\", and a few others) to generally be a \"disease\".  If you want to see more examples, you can explore the text of the corpus using the file browser to the left, or open files directly: \n",
    "\n",
    "* [data/NCBI/NCBI_corpus_training.txt](data/NCBI/NCBI_corpus_training.txt)\n",
    "* [data/NCBI/NCBI_corpus_testing.txt](data/NCBI/NCBI_corpus_testing.txt)\n",
    "* [data/NCBI/NCBI_corpus_development.txt](data/NCBI/NCBI_corpus_development.txt)\n",
    "\n",
    "We have already derived a dataset from this corpus. For NER, the dataset labels individual words as diseases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4e3973a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 3.0M\n",
      "-rw-r--r-- 1 root root 196K Jan 30 01:18 dev.tsv\n",
      "-rw-r--r-- 1 root root  63K Jan 30 01:30 labels_dev.txt\n",
      "-rw-r--r-- 1 root root  65K Jan 30 01:30 labels_test.txt\n",
      "-rw-r--r-- 1 root root 360K Jan 30 01:30 labels_train.txt\n",
      "-rw-r--r-- 1 root root 201K Jan 30 01:18 test.tsv\n",
      "-rw-r--r-- 1 root root 135K Jan 30 01:30 text_dev.txt\n",
      "-rw-r--r-- 1 root root 138K Jan 30 01:30 text_test.txt\n",
      "-rw-r--r-- 1 root root 760K Jan 30 01:30 text_train.txt\n",
      "-rw-r--r-- 1 root root 1.1M Jan 30 01:18 train.tsv\n"
     ]
    }
   ],
   "source": [
    "NER_DATA_DIR = f'{DATA_DIR}/NER'\n",
    "os.makedirs(os.path.join(DATA_DIR, 'NER'), exist_ok=True)\n",
    "\n",
    "# show downloaded files\n",
    "!ls -lh $NER_DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e04e6ad3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identification\tO\n",
      "of\tO\n",
      "APC2\tO\n",
      ",\tO\n",
      "a\tO\n",
      "homologue\tO\n",
      "of\tO\n",
      "the\tO\n",
      "adenomatous\tB-Disease\n",
      "polyposis\tI-Disease\n"
     ]
    }
   ],
   "source": [
    "!head $NER_DATA_DIR/train.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7ae47e",
   "metadata": {},
   "source": [
    "_Note:_ We can see that the abstract has been broken into sentences. Each sentence is then further parsed into words with labels that correspond to the original HTML-style tags in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0a652d",
   "metadata": {},
   "source": [
    "### Preprocess Data ###\n",
    "\n",
    "We need to convert these to a format that is compatible with NeMo token classification module. For convenience, we've provided the script for this conversion [here](https://github.com/NVIDIA/NeMo/blob/stable/examples/nlp/token_classification/data/import_from_iob_format.py). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6f188c35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE! Installing ujson may make loading annotations faster.\n",
      "[NeMo I 2025-01-30 01:46:03 import_from_iob_format:119] Processing data/NCBI/NER/train.tsv\n",
      "[NeMo I 2025-01-30 01:46:03 import_from_iob_format:124] Processing of the data/NCBI/NER/train.tsv is complete\n",
      "NOTE! Installing ujson may make loading annotations faster.\n",
      "[NeMo I 2025-01-30 01:46:07 import_from_iob_format:119] Processing data/NCBI/NER/dev.tsv\n",
      "[NeMo I 2025-01-30 01:46:07 import_from_iob_format:124] Processing of the data/NCBI/NER/dev.tsv is complete\n",
      "NOTE! Installing ujson may make loading annotations faster.\n",
      "[NeMo I 2025-01-30 01:46:12 import_from_iob_format:119] Processing data/NCBI/NER/test.tsv\n",
      "[NeMo I 2025-01-30 01:46:12 import_from_iob_format:124] Processing of the data/NCBI/NER/test.tsv is complete\n"
     ]
    }
   ],
   "source": [
    "# invoke the conversion script \n",
    "!python import_from_iob_format.py --data_file=$NER_DATA_DIR/train.tsv\n",
    "!python import_from_iob_format.py --data_file=$NER_DATA_DIR/dev.tsv\n",
    "!python import_from_iob_format.py --data_file=$NER_DATA_DIR/test.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9e28e5",
   "metadata": {},
   "source": [
    "Recall that the sentences and labels in the NER dataset map to each other with _inside, outside, beginning (IOB)_ tagging. Anything separated by white space is a word, including punctuation. This mechanism can be used in a general way for multiple named entity types:\n",
    "* B-{CHUNK_TYPE} – for the word in the Beginning chunk\n",
    "* I-{CHUNK_TYPE} – for words Inside the chunk\n",
    "* O – Outside any chunk\n",
    "\n",
    "In our case, we are only looking for \"disease\" as our entity (or chunk) type, so we don't need to identify beyond the three classes: I, O, and B.\n",
    "**Three classes**\n",
    "* B - Beginning of disease name\n",
    "* I - Inside word of disease name\n",
    "* O - Outside of all disease names\n",
    "\n",
    "As an example, for the first sentence we have the following mapping: \n",
    "\n",
    "```text\n",
    "Identification of APC2 , a homologue of the adenomatous polyposis coli tumour suppressor .\n",
    "O              O  O    O O O         O  O   B           I         I    I      O          O  \n",
    "```\n",
    "\n",
    "For comparison, the original corpus tags looked like:\n",
    "```html\n",
    "Identification of APC2, a homologue of the <category=\"Modifier\">adenomatous polyposis coli tumour</category> suppressor.\n",
    "```\n",
    "\n",
    "The beginning word of the tagged text, \"adenomatous\", is now IOB-tagged with a **B** (beginning) tag, the other parts of the disease, \"polyposis coli tumour\" tagged with **I** (inside) tags, and everything else tagged as **O** (outside)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f9e5ffdc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identification of APC2 , a homologue of the adenomatous polyposis coli tumour suppressor . \n",
      "O O O O O O O O B-Disease I-Disease I-Disease I-Disease O O \n"
     ]
    }
   ],
   "source": [
    "# preview dataset\n",
    "!head -n 1 $NER_DATA_DIR/text_train.txt\n",
    "!head -n 1 $NER_DATA_DIR/labels_train.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d768da0",
   "metadata": {},
   "source": [
    "## Fine-Tune a Pre-Trained Model for Custom Domain ##\n",
    "\n",
    "A name entity recognition model is typically comprised of a pre-trained [BERT](https://arxiv.org/pdf/1810.04805.pdf) model followed by a token classification layer. For training, we can use a configuration file to define the model. The configuration (config) file consists of several important sections, including: \n",
    "* **model**: All arguments that are related to the Model - language model, token classifier, optimizer and schedulers, datasets and any other related information\n",
    "* **trainer**: Any argument to be passed to PyTorch Lightning\n",
    "\n",
    "_Note:_ NeMo provides a template for creating the configuration file, which is recommended as a starting point, but you can create your own as long as it follows the required format. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fa7349",
   "metadata": {},
   "source": [
    "### Configuration File ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aad59abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define config path\n",
    "MODEL_CONFIG = \"token_classification_config.yaml\"\n",
    "WORK_DIR = \"WORK_DIR\"\n",
    "os.makedirs(WORK_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cdf86e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config file already exists\n"
     ]
    }
   ],
   "source": [
    "# download the model's configuration file \n",
    "BRANCH = 'main'\n",
    "config_dir = WORK_DIR + '/configs/'\n",
    "os.makedirs(config_dir, exist_ok=True)\n",
    "\n",
    "if not os.path.exists(config_dir + MODEL_CONFIG):\n",
    "    print('Downloading config file...')\n",
    "    wget.download(f'https://raw.githubusercontent.com/NVIDIA/NeMo/{BRANCH}/examples/nlp/token_classification/conf/' + MODEL_CONFIG, config_dir)\n",
    "else:\n",
    "    print ('config file already exists')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146b9f31",
   "metadata": {},
   "source": [
    "The config file for NER, `token_classification_config.yaml`, specifies model, training, and experiment management details, such as file locations, pretrained models, and hyperparameters. The YAML config file we downloaded provides default values for most of the parameters, but there are a few items that must be specified for this experiment.\n",
    "\n",
    "Each YAML section is a bit easier to view using the `omegaconf` package, which allows you to access and manipulate the configuration keys using a \"dot\" notation. We'll take a look at the details of each section using the `OmegaConf` tool. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e43cbb52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained_model: null\n",
      "trainer:\n",
      "  devices: 1\n",
      "  num_nodes: 1\n",
      "  max_epochs: 5\n",
      "  max_steps: -1\n",
      "  accumulate_grad_batches: 1\n",
      "  gradient_clip_val: 0.0\n",
      "  precision: 16\n",
      "  accelerator: gpu\n",
      "  enable_checkpointing: false\n",
      "  logger: false\n",
      "  log_every_n_steps: 1\n",
      "  val_check_interval: 1.0\n",
      "exp_manager:\n",
      "  exp_dir: null\n",
      "  name: token_classification_model\n",
      "  create_tensorboard_logger: true\n",
      "  create_checkpoint_callback: true\n",
      "model:\n",
      "  label_ids: null\n",
      "  class_labels:\n",
      "    class_labels_file: label_ids.csv\n",
      "  dataset:\n",
      "    data_dir: ???\n",
      "    class_balancing: null\n",
      "    max_seq_length: 128\n",
      "    pad_label: O\n",
      "    ignore_extra_tokens: false\n",
      "    ignore_start_end: false\n",
      "    use_cache: false\n",
      "    num_workers: 2\n",
      "    pin_memory: false\n",
      "    drop_last: false\n",
      "  train_ds:\n",
      "    text_file: text_train.txt\n",
      "    labels_file: labels_train.txt\n",
      "    shuffle: true\n",
      "    num_samples: -1\n",
      "    batch_size: 64\n",
      "  validation_ds:\n",
      "    text_file: text_dev.txt\n",
      "    labels_file: labels_dev.txt\n",
      "    shuffle: false\n",
      "    num_samples: -1\n",
      "    batch_size: 64\n",
      "  test_ds:\n",
      "    text_file: text_dev.txt\n",
      "    labels_file: labels_dev.txt\n",
      "    shuffle: false\n",
      "    num_samples: -1\n",
      "    batch_size: 64\n",
      "  tokenizer:\n",
      "    tokenizer_name: ${model.language_model.pretrained_model_name}\n",
      "    vocab_file: null\n",
      "    tokenizer_model: null\n",
      "    special_tokens: null\n",
      "  language_model:\n",
      "    pretrained_model_name: bert-base-uncased\n",
      "    lm_checkpoint: null\n",
      "    config_file: null\n",
      "    config: null\n",
      "  head:\n",
      "    num_fc_layers: 2\n",
      "    fc_dropout: 0.5\n",
      "    activation: relu\n",
      "    use_transformer_init: true\n",
      "  optim:\n",
      "    name: adam\n",
      "    lr: 5.0e-05\n",
      "    weight_decay: 0.0\n",
      "    sched:\n",
      "      name: WarmupAnnealing\n",
      "      warmup_steps: null\n",
      "      warmup_ratio: 0.1\n",
      "      last_epoch: -1\n",
      "      monitor: val_loss\n",
      "      reduce_on_plateau: false\n",
      "hydra:\n",
      "  run:\n",
      "    dir: .\n",
      "  job_logging:\n",
      "    root:\n",
      "      handlers: null\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "CONFIG_DIR = \"/dli/task/WORK_DIR/configs\"\n",
    "CONFIG_FILE = \"token_classification_config.yaml\"\n",
    "\n",
    "config=OmegaConf.load(CONFIG_DIR + \"/\" + CONFIG_FILE)\n",
    "\n",
    "# print the entire configuration file\n",
    "print(OmegaConf.to_yaml(config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da66c467",
   "metadata": {},
   "source": [
    "Notice that some config lines, including `model.dataset.data_dir`, have `???` in place of paths, this means that values for these fields are required to be specified by the user. Details about the model arguments can be found in the [documentation](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/nlp/token_classification.html#training-the-token-classification-model). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f7c6e9de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_ids: null\n",
      "class_labels:\n",
      "  class_labels_file: label_ids.csv\n",
      "dataset:\n",
      "  data_dir: data/NCBI/NER\n",
      "  class_balancing: null\n",
      "  max_seq_length: 128\n",
      "  pad_label: O\n",
      "  ignore_extra_tokens: false\n",
      "  ignore_start_end: false\n",
      "  use_cache: false\n",
      "  num_workers: 2\n",
      "  pin_memory: false\n",
      "  drop_last: false\n",
      "train_ds:\n",
      "  text_file: text_train.txt\n",
      "  labels_file: labels_train.txt\n",
      "  shuffle: true\n",
      "  num_samples: -1\n",
      "  batch_size: 64\n",
      "validation_ds:\n",
      "  text_file: text_dev.txt\n",
      "  labels_file: labels_dev.txt\n",
      "  shuffle: false\n",
      "  num_samples: -1\n",
      "  batch_size: 64\n",
      "test_ds:\n",
      "  text_file: text_dev.txt\n",
      "  labels_file: labels_dev.txt\n",
      "  shuffle: false\n",
      "  num_samples: -1\n",
      "  batch_size: 64\n",
      "tokenizer:\n",
      "  tokenizer_name: ${model.language_model.pretrained_model_name}\n",
      "  vocab_file: null\n",
      "  tokenizer_model: null\n",
      "  special_tokens: null\n",
      "language_model:\n",
      "  pretrained_model_name: bert-base-uncased\n",
      "  lm_checkpoint: null\n",
      "  config_file: null\n",
      "  config: null\n",
      "head:\n",
      "  num_fc_layers: 2\n",
      "  fc_dropout: 0.5\n",
      "  activation: relu\n",
      "  use_transformer_init: true\n",
      "optim:\n",
      "  name: adam\n",
      "  lr: 5.0e-05\n",
      "  weight_decay: 0.0\n",
      "  sched:\n",
      "    name: WarmupAnnealing\n",
      "    warmup_steps: null\n",
      "    warmup_ratio: 0.1\n",
      "    last_epoch: -1\n",
      "    monitor: val_loss\n",
      "    reduce_on_plateau: false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# in this exercise, train and dev datasets are located in the same folder under the default names, \n",
    "# so it is enough to add the path of the data directory to the config\n",
    "config.model.dataset.data_dir = os.path.join(DATA_DIR, 'NER')\n",
    "\n",
    "# print the model section\n",
    "print(OmegaConf.to_yaml(config.model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bd688d",
   "metadata": {},
   "source": [
    "_Note:_ The required `model.dataset.data_dir` argument (for token classification) has been modified. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc1e7e9",
   "metadata": {},
   "source": [
    "### Download Domain-Specific Pre-Trained Model ###\n",
    "\n",
    "For this token classification task, we can start with the pre-trained `BioMegatron` language model. The `BioMegatron` model is a domain-specific, BERT-like Megatron-LM model trained on large biomedical text corpus. Since the model was trained on domain-specific text, we can expect to have better performance compared to the general language model for identifying disease. \n",
    "\n",
    "_Note:_ There are alternatives of BioMegatron such as BioBERT. It's worth experimenting with different pre-trained models to find the one that provide optimal performance for a specific task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8120b3ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "megatron_bert_345m_cased\n",
      "megatron_bert_345m_uncased\n",
      "biomegatron345m_biovocab_50k_cased\n",
      "biomegatron345m_biovocab_50k_uncased\n",
      "biomegatron345m_biovocab_30k_cased\n",
      "biomegatron345m_biovocab_30k_uncased\n",
      "biomegatron-bert-345m-cased\n",
      "biomegatron-bert-345m-uncased\n"
     ]
    }
   ],
   "source": [
    "# import dependencies\n",
    "from nemo.collections.nlp.models.language_modeling.megatron_bert_model import MegatronBertModel\n",
    "\n",
    "# list available pre-trained models\n",
    "for model in MegatronBertModel.list_available_models(): \n",
    "    print(model.pretrained_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497494aa",
   "metadata": {},
   "source": [
    "To load the pretrained BERT LM model, we change the `model.language_mode` argument in the config as well as a few other arguments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b5e5dc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the specified above model parameters to the config\n",
    "MODEL_NAME='biomegatron345m_biovocab_30k_cased'\n",
    "# MODEL_NAME='biomegatron-bert-345m-cased'\n",
    "\n",
    "config.model.language_model.lm_checkpoint=None\n",
    "config.model.language_model.pretrained_model_name=MODEL_NAME\n",
    "config.model.tokenizer.tokenizer_name=None\n",
    "\n",
    "# use appropriate configurations based on GPU capacity\n",
    "config.model.dataset_max_seq_length=64\n",
    "config.model.train_ds.batch_size=32\n",
    "config.model.validation_ds.batch_size=32\n",
    "config.model.test_ds.batch_size=32\n",
    "\n",
    "# limit the number of epochs for this demonstration\n",
    "config.trainer.max_epochs=1\n",
    "# config.trainer.precision=16\n",
    "# config.trainer.amp_level='O1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c306b4cb",
   "metadata": {},
   "source": [
    "_Note:_ Once the `token_classification_config.yaml` file has been loaded into memory, changing the configuration file will require the `config` variable to be re-defined. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc62f80",
   "metadata": {},
   "source": [
    "Now, we are ready to initialize our model. During the model initialization call, the dataset and data loaders will be prepared for training and evaluation. Also, the pretrained BERT model will be downloaded, which can take up to a few minutes depending on the size of the chosen BERT model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05623e8f",
   "metadata": {},
   "source": [
    "#### Exercise # 1 - Instantiate Model and Trainer ####\n",
    "\n",
    "* Modify the `<FIXME>` to instantiate a `TokenClassificationModel` based on the configuration file and trainer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d56aa03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit None Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
      "[NeMo W 2025-01-30 01:59:15 lm_utils:91] biomegatron345m_biovocab_30k_cased is not in get_pretrained_lm_models_list(include_external=False), will be using AutoModel from HuggingFace.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-01-30 01:59:15 cloud:58] Found existing object /root/.cache/torch/NeMo/NeMo_1.20.0/BioMegatron345m-biovocab-30k-cased/5885010653185bba59bf489ff757bf09/BioMegatron345m-biovocab-30k-cased.nemo.\n",
      "[NeMo I 2025-01-30 01:59:15 cloud:64] Re-using file from: /root/.cache/torch/NeMo/NeMo_1.20.0/BioMegatron345m-biovocab-30k-cased/5885010653185bba59bf489ff757bf09/BioMegatron345m-biovocab-30k-cased.nemo\n",
      "[NeMo I 2025-01-30 01:59:15 common:913] Instantiating model from pre-trained checkpoint\n",
      "[NeMo I 2025-01-30 01:59:17 megatron_init:234] Rank 0 has data parallel group: [0]\n",
      "[NeMo I 2025-01-30 01:59:17 megatron_init:237] All data parallel group ranks: [[0]]\n",
      "[NeMo I 2025-01-30 01:59:17 megatron_init:238] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2025-01-30 01:59:17 megatron_init:246] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2025-01-30 01:59:17 megatron_init:247] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2025-01-30 01:59:17 megatron_init:257] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2025-01-30 01:59:17 megatron_init:261] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2025-01-30 01:59:17 megatron_init:262] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2025-01-30 01:59:17 megatron_init:276] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2025-01-30 01:59:17 megatron_init:288] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2025-01-30 01:59:17 megatron_init:294] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2025-01-30 01:59:17 megatron_init:295] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2025-01-30 01:59:17 megatron_init:296] All embedding group ranks: [[0]]\n",
      "[NeMo I 2025-01-30 01:59:17 megatron_init:297] Rank 0 has embedding rank: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-01-30 01:59:17 modelPT:244] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-01-30 01:59:17 tokenizer_utils:204] Getting Megatron tokenizer for pretrained model name: megatron-bert-345m-cased, custom vocab file: /tmp/tmp4h564uf2/f67afcc805164750a9eb2aa564aaf9a9_pubmed_merged-all-cased.vocab.txt, and merges file: None\n",
      "[NeMo I 2025-01-30 01:59:17 tokenizer_utils:130] Getting HuggingFace AutoTokenizer with pretrained_model_name: bert-large-cased, vocab_file: /tmp/tmp4h564uf2/f67afcc805164750a9eb2aa564aaf9a9_pubmed_merged-all-cased.vocab.txt, merges_files: None, special_tokens_dict: {}, and use_fast: False\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65f6c13d3dbf494696396751d59375ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4ef0b4fa82049c997cfd739b2828eb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01e7055b7f044a2482297902296ac9d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using eos_token, but it is not set yet.\n",
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-01-30 01:59:17 megatron_base_model:264] Padded vocab_size: 31104, original vocab_size: 31079, dummy tokens: 25.\n",
      "[NeMo I 2025-01-30 01:59:18 save_restore_connector:249] Model MegatronBertModel was successfully restored from /root/.cache/torch/NeMo/NeMo_1.20.0/BioMegatron345m-biovocab-30k-cased/5885010653185bba59bf489ff757bf09/BioMegatron345m-biovocab-30k-cased.nemo.\n",
      "[NeMo I 2025-01-30 01:59:18 token_classification_utils:118] Processing data/NCBI/NER/labels_train.txt\n",
      "[NeMo I 2025-01-30 01:59:18 token_classification_utils:154] Labels mapping {'O': 0, 'B-Disease': 1, 'I-Disease': 2} saved to : data/NCBI/NER/label_ids.csv\n",
      "[NeMo I 2025-01-30 01:59:18 token_classification_utils:163] Three most popular labels in data/NCBI/NER/labels_train.txt:\n",
      "[NeMo I 2025-01-30 01:59:18 data_preprocessing:194] label: 0, 124819 out of 136086 (91.72%).\n",
      "[NeMo I 2025-01-30 01:59:18 data_preprocessing:194] label: 2, 6122 out of 136086 (4.50%).\n",
      "[NeMo I 2025-01-30 01:59:18 data_preprocessing:194] label: 1, 5145 out of 136086 (3.78%).\n",
      "[NeMo I 2025-01-30 01:59:18 token_classification_utils:165] Total labels: 136086. Label frequencies - {0: 124819, 2: 6122, 1: 5145}\n",
      "[NeMo I 2025-01-30 01:59:18 token_classification_utils:174] Class Weights: {0: 0.36342223539685464, 2: 7.409670042469781, 1: 8.816715257531584}\n",
      "[NeMo I 2025-01-30 01:59:18 token_classification_utils:178] Class weights saved to data/NCBI/NER/labels_train_weights.p\n",
      "[NeMo I 2025-01-30 01:59:26 token_classification_dataset:123] Setting Max Seq length to: 128\n",
      "[NeMo I 2025-01-30 01:59:26 data_preprocessing:404] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2025-01-30 01:59:26 data_preprocessing:406] Min: 4 |                  Max: 150 |                  Mean: 30.271723122238587 |                  Median: 28.0\n",
      "[NeMo I 2025-01-30 01:59:26 data_preprocessing:412] 75 percentile: 38.00\n",
      "[NeMo I 2025-01-30 01:59:26 data_preprocessing:413] 99 percentile: 75.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-01-30 01:59:26 token_classification_dataset:152] 3 are longer than 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-01-30 01:59:26 token_classification_dataset:155] *** Example ***\n",
      "[NeMo I 2025-01-30 01:59:26 token_classification_dataset:156] i: 0\n",
      "[NeMo I 2025-01-30 01:59:26 token_classification_dataset:157] subtokens: [CLS] Identification of APC ##2 , a homologue of the adenoma ##to ##us polyp ##osis coli tumour suppressor . [SEP]\n",
      "[NeMo I 2025-01-30 01:59:26 token_classification_dataset:158] loss_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2025-01-30 01:59:26 token_classification_dataset:159] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2025-01-30 01:59:26 token_classification_dataset:160] subtokens_mask: 0 1 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2025-01-30 01:59:26 token_classification_dataset:162] labels: 0 0 0 0 0 0 0 0 0 0 1 1 1 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2025-01-30 01:59:26 token_classification_dataset:278] features saved to data/NCBI/NER/cached__text_train.txt__labels_train.txt__BertTokenizer_128_31079_-1\n",
      "[NeMo I 2025-01-30 01:59:26 token_classification_utils:118] Processing data/NCBI/NER/labels_dev.txt\n",
      "[NeMo I 2025-01-30 01:59:26 token_classification_utils:138] Using provided labels mapping {'O': 0, 'B-Disease': 1, 'I-Disease': 2}\n",
      "[NeMo I 2025-01-30 01:59:26 token_classification_utils:163] Three most popular labels in data/NCBI/NER/labels_dev.txt:\n",
      "[NeMo I 2025-01-30 01:59:26 data_preprocessing:194] label: 0, 22092 out of 23969 (92.17%).\n",
      "[NeMo I 2025-01-30 01:59:26 data_preprocessing:194] label: 2, 1090 out of 23969 (4.55%).\n",
      "[NeMo I 2025-01-30 01:59:26 data_preprocessing:194] label: 1, 787 out of 23969 (3.28%).\n",
      "[NeMo I 2025-01-30 01:59:26 token_classification_utils:165] Total labels: 23969. Label frequencies - {0: 22092, 2: 1090, 1: 787}\n",
      "[NeMo I 2025-01-30 01:59:27 token_classification_dataset:123] Setting Max Seq length to: 99\n",
      "[NeMo I 2025-01-30 01:59:27 data_preprocessing:404] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2025-01-30 01:59:27 data_preprocessing:406] Min: 4 |                  Max: 99 |                  Mean: 30.793066088840735 |                  Median: 28.0\n",
      "[NeMo I 2025-01-30 01:59:27 data_preprocessing:412] 75 percentile: 39.00\n",
      "[NeMo I 2025-01-30 01:59:27 data_preprocessing:413] 99 percentile: 75.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-01-30 01:59:27 token_classification_dataset:152] 0 are longer than 99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-01-30 01:59:27 token_classification_dataset:155] *** Example ***\n",
      "[NeMo I 2025-01-30 01:59:27 token_classification_dataset:156] i: 0\n",
      "[NeMo I 2025-01-30 01:59:27 token_classification_dataset:157] subtokens: [CLS] BRCA ##1 is secreted and exhibits properties of a gran ##in . [SEP]\n",
      "[NeMo I 2025-01-30 01:59:27 token_classification_dataset:158] loss_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2025-01-30 01:59:27 token_classification_dataset:159] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2025-01-30 01:59:27 token_classification_dataset:160] subtokens_mask: 0 1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2025-01-30 01:59:27 token_classification_dataset:162] labels: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2025-01-30 01:59:27 token_classification_dataset:278] features saved to data/NCBI/NER/cached__text_dev.txt__labels_dev.txt__BertTokenizer_128_31079_-1\n",
      "[NeMo I 2025-01-30 01:59:27 token_classification_utils:118] Processing data/NCBI/NER/labels_dev.txt\n",
      "[NeMo I 2025-01-30 01:59:27 token_classification_utils:138] Using provided labels mapping {'O': 0, 'B-Disease': 1, 'I-Disease': 2}\n",
      "[NeMo I 2025-01-30 01:59:27 token_classification_utils:160] data/NCBI/NER/labels_dev_label_stats.tsv found, skipping stats calculation.\n",
      "[NeMo I 2025-01-30 01:59:29 token_classification_dataset:123] Setting Max Seq length to: 99\n",
      "[NeMo I 2025-01-30 01:59:29 data_preprocessing:404] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2025-01-30 01:59:29 data_preprocessing:406] Min: 4 |                  Max: 99 |                  Mean: 30.793066088840735 |                  Median: 28.0\n",
      "[NeMo I 2025-01-30 01:59:29 data_preprocessing:412] 75 percentile: 39.00\n",
      "[NeMo I 2025-01-30 01:59:29 data_preprocessing:413] 99 percentile: 75.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-01-30 01:59:29 token_classification_dataset:152] 0 are longer than 99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-01-30 01:59:29 token_classification_dataset:155] *** Example ***\n",
      "[NeMo I 2025-01-30 01:59:29 token_classification_dataset:156] i: 0\n",
      "[NeMo I 2025-01-30 01:59:29 token_classification_dataset:157] subtokens: [CLS] BRCA ##1 is secreted and exhibits properties of a gran ##in . [SEP]\n",
      "[NeMo I 2025-01-30 01:59:29 token_classification_dataset:158] loss_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2025-01-30 01:59:29 token_classification_dataset:159] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2025-01-30 01:59:29 token_classification_dataset:160] subtokens_mask: 0 1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2025-01-30 01:59:29 token_classification_dataset:162] labels: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2025-01-30 01:59:29 token_classification_dataset:278] features saved to data/NCBI/NER/cached__text_dev.txt__labels_dev.txt__BertTokenizer_128_31079_-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-01-30 01:59:29 lm_utils:91] biomegatron345m_biovocab_30k_cased is not in get_pretrained_lm_models_list(include_external=False), will be using AutoModel from HuggingFace.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-01-30 01:59:29 cloud:58] Found existing object /root/.cache/torch/NeMo/NeMo_1.20.0/BioMegatron345m-biovocab-30k-cased/5885010653185bba59bf489ff757bf09/BioMegatron345m-biovocab-30k-cased.nemo.\n",
      "[NeMo I 2025-01-30 01:59:29 cloud:64] Re-using file from: /root/.cache/torch/NeMo/NeMo_1.20.0/BioMegatron345m-biovocab-30k-cased/5885010653185bba59bf489ff757bf09/BioMegatron345m-biovocab-30k-cased.nemo\n",
      "[NeMo I 2025-01-30 01:59:29 common:913] Instantiating model from pre-trained checkpoint\n",
      "[NeMo I 2025-01-30 01:59:30 megatron_init:234] Rank 0 has data parallel group: [0]\n",
      "[NeMo I 2025-01-30 01:59:30 megatron_init:237] All data parallel group ranks: [[0]]\n",
      "[NeMo I 2025-01-30 01:59:30 megatron_init:238] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2025-01-30 01:59:30 megatron_init:246] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2025-01-30 01:59:30 megatron_init:247] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2025-01-30 01:59:30 megatron_init:257] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2025-01-30 01:59:30 megatron_init:261] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2025-01-30 01:59:30 megatron_init:262] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2025-01-30 01:59:30 megatron_init:276] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2025-01-30 01:59:30 megatron_init:288] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2025-01-30 01:59:30 megatron_init:294] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2025-01-30 01:59:30 megatron_init:295] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2025-01-30 01:59:30 megatron_init:296] All embedding group ranks: [[0]]\n",
      "[NeMo I 2025-01-30 01:59:30 megatron_init:297] Rank 0 has embedding rank: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-01-30 01:59:30 modelPT:244] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-01-30 01:59:30 tokenizer_utils:204] Getting Megatron tokenizer for pretrained model name: megatron-bert-345m-cased, custom vocab file: /tmp/tmpsb0ljviq/f67afcc805164750a9eb2aa564aaf9a9_pubmed_merged-all-cased.vocab.txt, and merges file: None\n",
      "[NeMo I 2025-01-30 01:59:30 tokenizer_utils:130] Getting HuggingFace AutoTokenizer with pretrained_model_name: bert-large-cased, vocab_file: /tmp/tmpsb0ljviq/f67afcc805164750a9eb2aa564aaf9a9_pubmed_merged-all-cased.vocab.txt, merges_files: None, special_tokens_dict: {}, and use_fast: False\n"
     ]
    }
   ],
   "source": [
    "# create trainer and model instances\n",
    "from nemo.collections.nlp.models import TokenClassificationModel\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "trainer=pl.Trainer(**config.trainer)\n",
    "ner_model=TokenClassificationModel(cfg=config.model, trainer=trainer)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1cfecd9e",
   "metadata": {},
   "source": [
    "from nemo.collections.nlp.models import TokenClassificationModel\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "trainer=pl.Trainer(**config.trainer)\n",
    "ner_model=TokenClassificationModel(cfg=config.model, trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ba4df2",
   "metadata": {},
   "source": [
    "click ... to show solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97571dc6",
   "metadata": {},
   "source": [
    "### Exercise # 2 - Model Training ###\n",
    "\n",
    "* Modify the `<FIXME>` to train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72053614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start model training\n",
    "trainer.fit(ner_model)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a83991b5",
   "metadata": {},
   "source": [
    "trainer.fit(ner_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be61e042",
   "metadata": {},
   "source": [
    "click ... to show solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cf712f",
   "metadata": {},
   "source": [
    "### Exercise # 3 - Model Evaluation ###\n",
    "\n",
    "* Modify the `<FIXME>` to evaluate the model. \n",
    "\n",
    "To see how the model performs, we can generate prediction similar to the way we did it before and compare it with the labels. Alternatively, the `evaluate_from_file()` method enables us to evaluate the model given `text_file` and `labels_file`. Optionally, you can use the `add_confusion_matrix` to get a visual representation of the model performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7401478f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a subset of our dev data\n",
    "!head -n 100 $NER_DATA_DIR/text_dev.txt > $NER_DATA_DIR/sample_text_dev.txt\n",
    "!head -n 100 $NER_DATA_DIR/labels_dev.txt > $NER_DATA_DIR/sample_labels_dev.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6cf50c",
   "metadata": {},
   "source": [
    "Now, let's generate predictions for the provided text file. If labels file is also specified, the model will evaluate the predictions and plot confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18c61bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model performance on sample\n",
    "ner_model.half().evaluate_from_file(\n",
    "    text_file=os.path.join(NER_DATA_DIR, 'sample_text_dev.txt'),\n",
    "    labels_file=os.path.join(NER_DATA_DIR, 'sample_labels_dev.txt'),\n",
    "    output_dir=WORK_DIR,\n",
    "    add_confusion_matrix=True,\n",
    "    normalize_confusion_matrix=True,\n",
    "    batch_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "215826df",
   "metadata": {},
   "source": [
    "ner_model.half().evaluate_from_file(\n",
    "    text_file=os.path.join(NER_DATA_DIR, 'sample_text_dev.txt'),\n",
    "    labels_file=os.path.join(NER_DATA_DIR, 'sample_labels_dev.txt'),\n",
    "    output_dir=WORK_DIR,\n",
    "    add_confusion_matrix=True,\n",
    "    normalize_confusion_matrix=True,\n",
    "    batch_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfd2852",
   "metadata": {},
   "source": [
    "click ... to show solution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f7137d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart the kernel\n",
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33d2bee",
   "metadata": {},
   "source": [
    "**Well Done!** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2ad728",
   "metadata": {},
   "source": [
    "![DLI Header](images/DLI_Header.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
